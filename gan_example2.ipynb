{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dental-destruction",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "competitive-question",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mnist 데이터를 불러온다.\n",
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "welcome-responsibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 생성된 mnist 이미지를 8x8 Grid로 보여주는 plot 함수를 정의한다.\n",
    "def plot(samples):\n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    gs = gridspec.GridSpec(8, 8)\n",
    "    gs.update(wspace=0.05, hspace=0.05)\n",
    "\n",
    "    for i, sample in enumerate(samples):\n",
    "        ax = plt.subplot(gs[i])\n",
    "        plt.axis('off')\n",
    "        plt.imshow(sample.reshape(28, 28))\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "sapphire-closure",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 설정값 선언\n",
    "num_epoch = 100000\n",
    "batch_size = 64\n",
    "num_input = 28 * 28\n",
    "num_latent_variable = 100\n",
    "num_hidden = 128\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "lucky-shirt",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\oon06\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\compat\\v2_compat.py:88: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "# 플레이스 홀더 선언\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, num_input])               # 인풋 이미지\n",
    "z = tf.placeholder(tf.float32, [None, num_latent_variable])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "happy-removal",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Generator 변수 설정\n",
    "# 100 -> 128 -> 784\n",
    "with tf.variable_scope('generator'):\n",
    "    # 히든 레이어 파라미터 \n",
    "    G_W1 = tf.Variable(tf.random_normal(shape=[num_latent_variable, num_hidden], stddev=5e-2))   \n",
    "    G_b1 = tf.Variable(tf.constant(0.1, shape=[num_hidden]))\n",
    "    # 아웃풋 레이어 파라미터\n",
    "    G_W2 = tf.Variable(tf.random_normal(shape=[num_hidden, num_input], stddev=5e-2))   \n",
    "    G_b2 = tf.Variable(tf.constant(0.1, shape=[num_input]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "modified-freeware",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator 변수 설정\n",
    "# 784 -> 128 -> 1\n",
    "with tf.variable_scope('discriminator'):\n",
    "    # 히든 레이어 파라미터\n",
    "    D_W1 = tf.Variable(tf.random_normal(shape=[num_input, num_hidden], stddev=5e-2))   \n",
    "    D_b1 = tf.Variable(tf.constant(0.1, shape=[num_hidden]))\n",
    "    # 아웃풋 레이어 파라미터\n",
    "    D_W2 = tf.Variable(tf.random_normal(shape=[num_hidden, 1], stddev=5e-2))   \n",
    "    D_b2 = tf.Variable(tf.constant(0.1, shape=[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "working-messenger",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator를 생성하는 함수 정의\n",
    "# Inputs: \n",
    "#   X : 인풋 Latent Variable \n",
    "# Output: \n",
    "#   generated_mnist_image : 생성된 MNIST 이미지 \n",
    "\n",
    "def build_generator(X):\n",
    "    hidden_layer = tf.nn.relu((tf.matmul(X, G_W1) + G_b1))\n",
    "    output_layer = tf.matmul(hidden_layer, G_W2) + G_b2\n",
    "    generated_mnist_image = tf.nn.sigmoid(output_layer)\n",
    "\n",
    "    return generated_mnist_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "weighted-visiting",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator를 생성하는 함수 정의\n",
    "# Inputs: \n",
    "#   X : 인풋 이미지 \n",
    "# Output: \n",
    "#   predicted_value : Discriminator가 판단한 True(1) or Fake(0) \n",
    "#   logits : sigmoid를 씌우기전의 출력값 \n",
    "\n",
    "def build_discriminator(X):\n",
    "    hidden_layer = tf.nn.relu((tf.matmul(X, D_W1) + D_b1))\n",
    "    logits = tf.matmul(hidden_layer, D_W2) + D_b2\n",
    "    predicted_value = tf.nn.sigmoid(logits)\n",
    "\n",
    "    return predicted_value, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "adjusted-assignment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 생성자(Generator) 선언\n",
    "G = build_generator(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "prescription-india",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 구분자(Discriminator)를 선언합니다.\n",
    "D_real, D_real_logits = build_discriminator(X)  # D(x)\n",
    "D_fake, D_fake_logits = build_discriminator(G)  # D(G(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "worth-badge",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator의 손실 함수 정의\n",
    "d_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_real_logits, labels=tf.ones_like(D_real_logits)))    # log(D(x))\n",
    "d_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_fake_logits, labels=tf.zeros_like(D_fake_logits)))   # log(1-D(G(z)))\n",
    "d_loss = d_loss_real + d_loss_fake  # log(D(x)) + log(1-D(G(z)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "potential-schema",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator의 손실 함수 정의\n",
    "g_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_fake_logits, labels=tf.ones_like(D_fake_logits)))         # log(D(G(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "quick-spectrum",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 파라미터를 Discriminator와 관련된 파라미터와 Generator와 관련된 파라미터로 나눈다.\n",
    "tvar = tf.trainable_variables()\n",
    "dvar = [var for var in tvar if 'discriminator' in var.name]\n",
    "gvar = [var for var in tvar if 'generator' in var.name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "jewish-poison",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator와 Generator의 Optimizer 정의\n",
    "d_train_step = tf.train.AdamOptimizer(learning_rate).minimize(d_loss, var_list=dvar)\n",
    "g_train_step = tf.train.AdamOptimizer(learning_rate).minimize(g_loss, var_list=gvar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "shaped-bristol",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\oon06\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3331, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-21-d047a2ec15dc>\", line 13, in <module>\n",
      "    batch_X, _ = mnist.compat.v1.train.next_batch(batch_size)\n",
      "AttributeError: module 'tensorflow_core.python.keras.api._v2.keras.datasets.mnist' has no attribute 'compat'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\oon06\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2044, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'AttributeError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\oon06\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1151, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"C:\\Users\\oon06\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 319, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\oon06\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 353, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"C:\\Users\\oon06\\Anaconda3\\lib\\inspect.py\", line 1502, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"C:\\Users\\oon06\\Anaconda3\\lib\\inspect.py\", line 1460, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"C:\\Users\\oon06\\Anaconda3\\lib\\inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"C:\\Users\\oon06\\Anaconda3\\lib\\inspect.py\", line 733, in getmodule\n",
      "    if ismodule(module) and hasattr(module, '__file__'):\n",
      "  File \"C:\\Users\\oon06\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\n",
      "    module = self._load()\n",
      "  File \"C:\\Users\\oon06\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\n",
      "    module = _importlib.import_module(self.__name__)\n",
      "  File \"C:\\Users\\oon06\\Anaconda3\\lib\\importlib\\__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 965, in _find_and_load_unlocked\n",
      "ModuleNotFoundError: No module named 'tensorflow_core.estimator'\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow_core.python.keras.api._v2.keras.datasets.mnist' has no attribute 'compat'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "# 생성된 이미지들을 저장할 generated_outputs 폴더 생성\n",
    "num_img = 0\n",
    "if not os.path.exists('generated_output/'):\n",
    "    os.makedirs('generated_output/')\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # 변수들에 초기값 할당\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "ppp/z..,.......................................................................................................................................\n",
    "    # num_epoch 횟수만큼 최적화 수행\n",
    "    for i in range(num_epoch):\n",
    "        # MNIST 이미지를 batch_size만큼 불러온다.\n",
    "        batch_X, _ = mnist.train.next_batch(batch_size)\n",
    "        # Latent Variable의 인풋으로 사용할 noise를 Uniform Distribution에서 batch_size만큼 샘플링\n",
    "        batch_noise = np.random.uniform(-1., 1., [batch_size, 100])\n",
    "\n",
    "        # 500번 반복할때마다 생성된 이미지 저장\n",
    "        if i % 500 == 0:\n",
    "            samples = sess.run(G, feed_dict={z: np.random.uniform(-1., 1., [64, 100])})\n",
    "            fig = plot(samples)\n",
    "            plt.savefig('generated_output/%s.png' % str(num_img).zfill(3), bbox_inches='tight')\n",
    "            num_img += 1\n",
    "            plt.close(fig)\n",
    "\n",
    "        # Discriminator 최적화를 수행하고 Discriminator의 손실함수를 return\n",
    "        _, d_loss_print = sess.run([d_train_step, d_loss], feed_dict={X: batch_X, z: batch_noise})\n",
    "\n",
    "        # Generator 최적화를 수행하고 Generator 손실함수를 return\n",
    "        _, g_loss_print = sess.run([g_train_step, g_loss], feed_dict={z: batch_noise})\n",
    "\n",
    "        # 100번 반복할때마다 Discriminator의 손실함수와 Generator 손실함수를 출력\n",
    "        if i % 100 == 0:\n",
    "            print('반복(Epoch): %d, Generator 손실함수(g_loss): %f, Discriminator 손실함수(d_loss): %f' % (i, g_loss_print, d_loss_print))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assumed-farmer",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
